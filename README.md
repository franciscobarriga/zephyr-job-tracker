# ğŸŒªï¸ Zephyr - Job Application Tracker

Automated job tracking with LinkedIn scraping, built with FastAPI and Supabase.

## âœ¨ Features

- ğŸ” **Secure Authentication** - Supabase-powered user accounts
- ğŸŒ **Persistent Sessions** - Stay logged in across page refreshes
- ğŸ¤– **Automated Scraping** - LinkedIn job scraper with Playwright
- ğŸ“Š **Analytics Dashboard** - Track applications, stats, and activity
- ğŸ” **Custom Searches** - Define keywords, locations, and filters
- ğŸ‘¥ **Multi-User** - Each user has their own private dashboard
- ğŸ¨ **Modern UI** - Responsive Bootstrap 5 design

## ğŸš€ Quick Start

### 1. Clone & Setup

```bash
git clone <your-repo>
cd zephyr
python3 -m venv venv
source venv/bin/activate
pip install -r requirements.txt
```

### 2. Install Playwright Browsers

```bash
playwright install chromium
```

### 3. Configure Environment

Create `.env` file with your Supabase credentials:

```env
SUPABASE_URL=your_supabase_url
SUPABASE_ANON_KEY=your_anon_key
SUPABASE_SERVICE_KEY=your_service_key
SECRET_KEY=your-secret-key-for-sessions
```

### 4. Run the Application

**Easy way:**
```bash
./start.sh
```

**Manual way:**
```bash
python run.py
```

Visit: **http://localhost:8000**

## ğŸ“ Project Structure

```
zephyr/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py              # FastAPI application
â”‚   â”œâ”€â”€ auth.py              # Authentication logic
â”‚   â”œâ”€â”€ routes/              # Route handlers
â”‚   â”‚   â”œâ”€â”€ auth.py          # Login/signup/logout
â”‚   â”‚   â”œâ”€â”€ dashboard.py     # Main dashboard
â”‚   â”‚   â”œâ”€â”€ jobs.py          # Job listings
â”‚   â”‚   â””â”€â”€ search.py        # Search configurations
â”‚   â”œâ”€â”€ templates/           # Jinja2 HTML templates
â”‚   â””â”€â”€ static/              # CSS, JS, images
â”œâ”€â”€ scraper.py               # LinkedIn job scraper
â”œâ”€â”€ run.py                   # Application starter
â”œâ”€â”€ start.sh                 # Quick start script
â”œâ”€â”€ requirements.txt         # Python dependencies
â””â”€â”€ .env                     # Configuration (not in git)
```

## ğŸ”§ Usage

### Running the Web App

```bash
./start.sh
```

Then open http://localhost:8000 in your browser.

### Running the Scraper

The scraper runs automatically via GitHub Actions every 6 hours, or run it manually:

```bash
python scraper.py
```

### Managing Search Configs

1. Log in to the web app
2. Navigate to "Search Configs"
3. Add your search criteria (keywords, location, etc.)
4. Toggle active/inactive as needed

The scraper only runs for active configurations.

## ğŸ—ï¸ Architecture

- **Frontend**: Jinja2 templates + Bootstrap 5
- **Backend**: FastAPI (Python async framework)
- **Database**: Supabase (PostgreSQL)
- **Auth**: Supabase Auth with session cookies
- **Scraper**: Playwright (headless browser)

## ğŸš¢ Deployment

### Render.com (Recommended)

1. Connect your GitHub repository
2. Create a new Web Service
3. Build command: `pip install -r requirements.txt && playwright install chromium`
4. Start command: `python run.py`
5. Add environment variables

### Railway.app

1. Connect repository
2. Add environment variables
3. Railway auto-deploys

### Fly.io

1. `flyctl launch`
2. `flyctl secrets set SUPABASE_URL=...`
3. `flyctl deploy`

## ğŸ›¡ï¸ Security

- âœ… Environment variables for secrets
- âœ… Supabase Row Level Security (RLS)
- âœ… Session-based authentication
- âœ… Input validation

## ğŸ“ To-Do

- [ ] Job application automation
- [ ] More job boards (Indeed, Glassdoor)
- [ ] Email notifications
- [ ] Export to CSV/PDF
- [ ] Chrome extension for one-click apply
- [ ] Advanced filtering and sorting
- [ ] Application status tracking

---

Built with â¤ï¸ using FastAPI, Supabase, and Playwright
 
Peepee